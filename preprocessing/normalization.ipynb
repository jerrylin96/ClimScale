{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c891cb0",
   "metadata": {},
   "source": [
    "# Normalization \n",
    "\n",
    "The normalization for training and validation dataset. This normalization funtions (class) will be called in the data generator to normalize data in every batch. In this normalization script, we need first to calculate the mean and std of one year dataset. Since the calculation will cost a lot of time. We find two relative efficiently apporaches to deal with normalization process.\n",
    "\n",
    "1. Subsample from all datasets. This approach is a approximation to the global mean and std, which was implemented in CBRAIN by Stephen Rasp.\n",
    "\n",
    "2. Divide all datasets into several subsets. Calculate mean of every subsets (sub_mean), then calculate the global mean and std based on several sub_means. This approach is the same as calculated directly with all datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be267ce",
   "metadata": {},
   "source": [
    "Objective: \n",
    "    \n",
    "    the mean and std of one year datasets.\n",
    "\n",
    " Function: \n",
    "\n",
    "    1. Divide one year datasets into 12 months\n",
    "    2. Calculate 12 month mean values, save the mean and release the RAM after calculation of the mean\n",
    "    3. Average 12 means to get the global mean\n",
    "    4. Based on the equation to calculate the global std. \n",
    "    \n",
    "    \n",
    "$S = \\sqrt{\\frac{\\sum (x_{i} - \\bar{x})^2}{n-1} } $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5de71",
   "metadata": {},
   "source": [
    "## 1. Subsample normalization\n",
    "\n",
    "We subsampled first three days from every month to calculate the approixmate mean and std, which was implemented in the CBRAIN by Stephen Rasp. In this approach, the normalization process is the same as Jerry's previous workflow. So we don't need to change anything.\n",
    "> Note that in Jerry's workflow he sampled the data spatially with the arguement `space=8`. We should cancel spatial sampling (set `space=1`) to ensure a sufficient number of samples to calculate mean and std. We don't need to change the `space` at this time since the training data also be sampled. So We just follow Jerry's workflow without change anything. If we want to use all data for training, we should modify the normalization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c79a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eae81aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/ocean/projects/atm200007p/jlin96/longSPrun/'\n",
    "spData_train_input_3d = ! ls {data_path} | grep '0[123]-00000.nc' # first three days of every month\n",
    "spData_train_input_3d = [data_path + s for s in spData_train_input_3d[:36]]\n",
    "spData_train_input_3d = xr.open_mfdataset(spData_train_input_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49c2926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded in data\n",
      "starting for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [06:55<00:00,  6.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nntbp\n",
      "(30, 1727, 64, 128)\n",
      "nnqbp\n",
      "(30, 1727, 64, 128)\n",
      "lhflx\n",
      "(1, 1727, 64, 128)\n",
      "shflx\n",
      "(1, 1727, 64, 128)\n",
      "ps\n",
      "(1, 1727, 64, 128)\n",
      "solin\n",
      "(1, 1727, 64, 128)\n",
      "newhum\n",
      "(30, 1727, 64, 128)\n",
      "oldhum\n",
      "(30, 1727, 64, 128)\n",
      "nnInput\n",
      "(64, 1727, 64, 16)\n",
      "Mean relative humidity conversion error: 0.004416230272216098\n",
      "Variance for relative humidity conversion error: 0.000562019650883235\n",
      "nntbp.shape: (30, 1727, 64, 128)\n",
      "nnqbp.shape: (30, 1727, 64, 128)\n",
      "lhflx.shape: (1, 1727, 64, 128)\n",
      "shflx.shape: (1, 1727, 64, 128)\n",
      "ps.shape: (1, 1727, 64, 128)\n",
      "solin.shape: (1, 1727, 64, 128)\n",
      "newhum.shape: (30, 1727, 64, 128)\n",
      "oldhum.shape: (30, 1727, 64, 128)\n",
      "nnInput.shape: (64, 1727, 64, 16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subsample data with `space=8`\n",
    "spData_train_input_3d = combine_arrays(make_nn_input(spData_train_input_3d, family = \"relative\", print_diagnostics = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a559a38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1767424)\n"
     ]
    }
   ],
   "source": [
    "# reshape\n",
    "spData_train_input_3d = reshape_input(spData_train_input_3d)\n",
    "print(spData_train_input_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfaf212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: \n",
      "(1767424, 64)\n",
      "INP_SUB shape: \n",
      "(64, 1)\n",
      "INP_DIV shape: \n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_3d, inp_sub_3d, inp_div_3d = normalize_input_train(X_train = spData_train_input_3d, save_files = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc3353bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_train_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b66722",
   "metadata": {},
   "source": [
    "## Normalization with all dataset\n",
    "In order to estimate the errors between sampling normalization and global normalization, we need to calculate the mean and std using all data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6db39bac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66155/3455544380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspData_train_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspData_train_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspData_train_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_mfdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspData_train_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspData_train_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/practice-env/lib/python3.9/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0;34mf\"{type(self).__name__!r} object has no attribute {name!r}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "spData_train_input = ! ls {data_path} | grep 'h1.0000-*'\n",
    "spData_train_input = [data_path + s for s in spData_train_input]\n",
    "spData_train_input = xr.open_mfdataset(spData_train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33cee7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded in data\n",
      "starting for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [1:07:18<00:00, 63.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nntbp\n",
      "(30, 17519, 64, 128)\n",
      "nnqbp\n",
      "(30, 17519, 64, 128)\n",
      "lhflx\n",
      "(1, 17519, 64, 128)\n",
      "shflx\n",
      "(1, 17519, 64, 128)\n",
      "ps\n",
      "(1, 17519, 64, 128)\n",
      "solin\n",
      "(1, 17519, 64, 128)\n",
      "newhum\n",
      "(30, 17519, 64, 128)\n",
      "oldhum\n",
      "(30, 17519, 64, 128)\n",
      "nnInput\n",
      "(64, 17519, 64, 16)\n",
      "Mean relative humidity conversion error: 0.004461620695778564\n",
      "Variance for relative humidity conversion error: 0.0005681880569581219\n",
      "nntbp.shape: (30, 17519, 64, 128)\n",
      "nnqbp.shape: (30, 17519, 64, 128)\n",
      "lhflx.shape: (1, 17519, 64, 128)\n",
      "shflx.shape: (1, 17519, 64, 128)\n",
      "ps.shape: (1, 17519, 64, 128)\n",
      "solin.shape: (1, 17519, 64, 128)\n",
      "newhum.shape: (30, 17519, 64, 128)\n",
      "oldhum.shape: (30, 17519, 64, 128)\n",
      "nnInput.shape: (64, 17519, 64, 16)\n",
      "\n",
      "(64, 17938432)\n"
     ]
    }
   ],
   "source": [
    "# reshape\n",
    "spData_train_input = combine_arrays(make_nn_input(spData_train_input, family = \"relative\", print_diagnostics = True))\n",
    "spData_train_input = reshape_input(spData_train_input)\n",
    "print(spData_train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fee38619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 1767424), (64, 17938432))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spData_train_input_3d.shape, spData_train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2c901be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: \n",
      "(17938432, 64)\n",
      "INP_SUB shape: \n",
      "(64, 1)\n",
      "INP_DIV shape: \n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, inp_sub, inp_div = normalize_input_train(X_train = spData_train_input, save_files = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c271903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference\n",
    "diff_sub = (inp_sub - inp_sub_3d)\n",
    "diff_div = (inp_div - inp_div_3d)\n",
    "\n",
    "# relative error\n",
    "diff_sub_re = diff_sub / inp_sub\n",
    "diff_div_re = diff_div / inp_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c8f14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_path = '../training/norm_files/'\n",
    "np.savetxt(norm_path + \"inp_sub_3d.txt\", inp_sub_3d, delimiter=',')\n",
    "np.savetxt(norm_path + \"inp_div_3d.txt\", inp_div_3d, delimiter=',')\n",
    "np.savetxt(norm_path + \"inp_sub.txt\", inp_sub, delimiter=',')\n",
    "np.savetxt(norm_path + \"inp_div.txt\", inp_div, delimiter=',')\n",
    "np.savetxt(norm_path + \"inp_sub_re.txt\", diff_sub_re, delimiter=',')\n",
    "np.savetxt(norm_path + \"inp_div_re.txt\", diff_div_re, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a95858e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.64231081e-03],\n",
       "       [ 2.44049322e-03],\n",
       "       [ 3.79727147e-03],\n",
       "       [-1.61518298e-03],\n",
       "       [-3.68291146e-03],\n",
       "       [-2.28174110e-03],\n",
       "       [-5.82471073e-04],\n",
       "       [ 1.65606086e-03],\n",
       "       [ 2.40968722e-06],\n",
       "       [-1.67228977e-03],\n",
       "       [-1.19925376e-03],\n",
       "       [ 9.73574745e-04],\n",
       "       [ 5.31810213e-03],\n",
       "       [ 8.89999689e-03],\n",
       "       [ 1.18258173e-02],\n",
       "       [ 1.54700750e-02],\n",
       "       [ 1.72744807e-02],\n",
       "       [ 1.73006562e-02],\n",
       "       [ 1.65300093e-02],\n",
       "       [ 1.50818047e-02],\n",
       "       [ 1.30678423e-02],\n",
       "       [ 1.03079726e-02],\n",
       "       [ 8.49296786e-03],\n",
       "       [ 7.49021928e-03],\n",
       "       [ 6.75449250e-03],\n",
       "       [ 6.17895239e-03],\n",
       "       [ 6.00480277e-03],\n",
       "       [ 6.20741673e-03],\n",
       "       [ 6.95788353e-03],\n",
       "       [ 8.48935096e-03],\n",
       "       [-7.80933463e-03],\n",
       "       [-8.98542308e-03],\n",
       "       [ 3.84467087e-03],\n",
       "       [-7.52525962e-02],\n",
       "       [-5.91047126e-02],\n",
       "       [-4.23495474e-02],\n",
       "       [-2.47092095e-02],\n",
       "       [-6.69444050e-03],\n",
       "       [-1.67327332e-02],\n",
       "       [-2.03740981e-02],\n",
       "       [-1.34517297e-02],\n",
       "       [-6.80307512e-03],\n",
       "       [ 5.50044205e-04],\n",
       "       [ 2.22082371e-03],\n",
       "       [-8.75160105e-04],\n",
       "       [-4.53794919e-03],\n",
       "       [-6.74937170e-03],\n",
       "       [-5.88823337e-03],\n",
       "       [-2.16134458e-03],\n",
       "       [ 1.09471159e-03],\n",
       "       [ 3.43087751e-04],\n",
       "       [-6.84424346e-03],\n",
       "       [-2.04153806e-02],\n",
       "       [-2.04484500e-02],\n",
       "       [-1.05090517e-02],\n",
       "       [ 1.10178704e-03],\n",
       "       [ 1.38664469e-02],\n",
       "       [ 1.41526903e-02],\n",
       "       [ 1.55891823e-02],\n",
       "       [ 1.85643573e-02],\n",
       "       [-5.91569588e-03],\n",
       "       [-1.43424247e-05],\n",
       "       [ 9.92616342e-03],\n",
       "       [ 2.15371131e-02]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_div_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c46b8",
   "metadata": {},
   "source": [
    "## 2. Calculated from month average \n",
    "1. Load month files separately, calculate the mean for each month, save mean values and del dataset in RAM \n",
    "2. Avarage the month means to get global mean\n",
    "3. Calculate the std based on equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e246223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu_m = []\n",
    "for i in range(1,13):\n",
    "    spData_train_input_m = 'spData_train_input_' + i + 'm'\n",
    "    spData_train_input_m = combine_arrays(make_nn_input(load_data(month = i, year = 0, data_path = data_path), family = \"relative\", print_diagnostics = True))\n",
    "    spData_train_input_m = reshape_input(spData_train_input_m)\n",
    "    X_train, inp_sub, inp_div = normalize_input_train(X_train = spData_train_input_m, save_files = False)\n",
    "    train_mu_m.append(inp_sub)\n",
    "    train_mu_m = np.array(train_mu_m)\n",
    "    del X_train\n",
    "    \n",
    "train_mu = np.mean(train_mu_m, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input_train(X_train, reshaped = True, norm = \"standard\", save_files = False, norm_path = \"../training/norm_files/\", save_path = \"../training/training_data/\"):\n",
    "    if reshaped:\n",
    "        train_mu = np.mean(X_train, axis = 1)[:, np.newaxis]\n",
    "        train_std = np.std(X_train, axis = 1)[:, np.newaxis]\n",
    "        train_min = X_train.min(axis = 1)[:, np.newaxis]\n",
    "        train_max = X_train.max(axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    else:\n",
    "        train_mu = np.mean(X_train, axis = (1,2,3))[:, np.newaxis]\n",
    "        train_std = np.std(X_train, axis = (1,2,3))[:, np.newaxis]\n",
    "        train_min = X_train.min(axis = (1,2,3))[:, np.newaxis]\n",
    "        train_max = X_train.max(axis = (1,2,3))[:, np.newaxis]\n",
    "        \n",
    "    if norm == \"standard\":\n",
    "        inp_sub = train_mu\n",
    "        inp_div = train_std\n",
    "        \n",
    "    elif norm == \"range\":\n",
    "        inp_sub = train_min\n",
    "        inp_div = train_max - train_min\n",
    "        \n",
    "    #normalizing\n",
    "    X_train = ((X_train - inp_sub)/inp_div).transpose()\n",
    "    #normalized\n",
    "    \n",
    "    print(\"X_train shape: \")\n",
    "    print(X_train.shape)\n",
    "    print(\"INP_SUB shape: \")\n",
    "    print(inp_sub.shape)\n",
    "    print(\"INP_DIV shape: \")\n",
    "    print(inp_div.shape)\n",
    "    \n",
    "    if save_files:\n",
    "        with open(save_path + \"train_input.npy\", 'wb') as f:\n",
    "            np.save(f, np.float32(X_train))\n",
    "        np.savetxt(norm_path + \"inp_sub.txt\", inp_sub, delimiter=',')\n",
    "        np.savetxt(norm_path + \"inp_div.txt\", inp_div, delimiter=',')\n",
    "    \n",
    "    return X_train, inp_sub, inp_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2004e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
